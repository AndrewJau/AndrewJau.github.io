---
layout: post
title:  "ML笔记"
date:   2018-05-30 09:42:23 +0800
categories: Notes
---

## Instruction
1. Supervised learning: 给定一个数据集，并且已知正确的输出，求预测值。分为regression (连续的) 和classification (0或1)。

2. Unsupervised learning: 给定一个数据集，但是没有预测反馈，可以通过数据和不同的表现给出数据的结构。分为clustering (数据的变量包含一些关系，通过相关性分析数据) 和non-clustering (在混乱的环境下找到数据的结构)

## Cost Function
1. Cost function: 对于线性回归，损失函数就是实际值和线性方程预测值的平方差
![avatar](https://raw.githubusercontent.com/AndrewJau/Stock/master/cf.jpg)

2. 我们要做的就是给定一个假设函数h(x) = ø0+ø1*x，通过寻找最佳的ø0和ø1的值来最小化损失函数（代价函数）的结果。

## Gradient Descent
1. 梯度下降就是通过求导的方式，找到每一个时刻下降最快的方向，一点点的修改ø的值，并循环操作直到收敛。
![avatar](https://raw.githubusercontent.com/AndrewJau/Stock/master/梯度下降.jpg)
公式中α的值代表learning rate，代表每次下降的多少（大步或者小碎步）

2. 可以做feature scaling把feature的值缩小到-1~1的区间，可以加速损失函数的收敛速度

3. 选feature的时候，如果有需要次幂形式的话，尽量选择最终能维持上升的幂函数，如根号和3次方（以此类推）

4. ![avatar](https://raw.githubusercontent.com/AndrewJau/Stock/master/ø求解公式.jpg)